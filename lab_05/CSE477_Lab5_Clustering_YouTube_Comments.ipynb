{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed3245a",
   "metadata": {},
   "source": [
    "\n",
    "# CSE477 Lab 5: Clustering YouTube Comments — Step-by-Step (Colab Ready)\n",
    "\n",
    "**What this notebook does**\n",
    "- Loads your cleaned comments from Google Drive\n",
    "- Converts text → TF‑IDF vectors\n",
    "- Runs **K-Means** with elbow + helpful metrics\n",
    "- Runs **DBSCAN** with guided `eps` tuning\n",
    "- Gives **better visualizations** (term bars, 2D scatter, sample comments)\n",
    "- Exports results to a CSV with cluster labels\n",
    "\n",
    "**How to use (Google Colab)**\n",
    "1. Upload this notebook to **Google Colab**.\n",
    "2. Run the **Install & Setup** cell to mount Drive.\n",
    "3. Make sure your file path is correct:  \n",
    "   `DATA_PATH = '/content/drive/MyDrive/CSE477/cleaned_comments.csv'`\n",
    "4. Run cells **top → bottom**. Fill the short reflections where asked.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07b06bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Install & Setup (Colab) ---\n",
    "# If you're on Colab, this will install any missing packages.\n",
    "!pip -q install pandas numpy matplotlib seaborn scikit-learn umap-learn kneed\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os, ast, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from textwrap import shorten\n",
    "plt.rcParams['figure.dpi'] = 120  # a bit crisper visuals\n",
    "\n",
    "# >>>> EDIT ME if your file path is different <<<<\n",
    "DATA_PATH = '/content/drive/MyDrive/CSE477/cleaned_comments.csv'\n",
    "\n",
    "print(\"Using file:\", DATA_PATH)\n",
    "assert os.path.exists(DATA_PATH), \"Path doesn't exist. Fix DATA_PATH above.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e424ca5",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1 — Data Validation and Hypothesis\n",
    "- Load your dataset (`.csv` or `.txt`).\n",
    "- Report its shape with `df.shape`.\n",
    "- **Critical Prompt #1** (answer just below the output):  \n",
    "  Based on your number of comments, what do you expect about cluster **quality**?  \n",
    "  Small datasets run, but do they yield meaningful topics?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913caae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset (CSV or TXT)\n",
    "if DATA_PATH.lower().endswith(\".csv\"):\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "else:\n",
    "    df = pd.read_table(DATA_PATH)\n",
    "\n",
    "print(f\"My dataset contains {df.shape[0]} comments and {df.shape[1]} columns.\")\n",
    "display(df.head())\n",
    "print(\"Columns:\", list(df.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b843fcba",
   "metadata": {},
   "source": [
    "\n",
    "**Critical Prompt #1 (your short answer, 1–2 sentences):**  \n",
    "> _Write your initial hypothesis about cluster quality here._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd783b",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2 — Text → Vector (TF‑IDF)\n",
    "Handles either `cleaned_tokens` (list/str of tokens) **or** `cleaned_text`.  \n",
    "If neither is found, it tries to infer a text-like column automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40d6e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def build_corpus(df):\n",
    "    if 'cleaned_tokens' in df.columns:\n",
    "        def to_tokens(x):\n",
    "            if isinstance(x, list):\n",
    "                return x\n",
    "            if isinstance(x, str):\n",
    "                # try to parse list-like strings, else fallback to split\n",
    "                try:\n",
    "                    v = ast.literal_eval(x)\n",
    "                    if isinstance(v, list):\n",
    "                        return [str(t) for t in v]\n",
    "                except Exception:\n",
    "                    pass\n",
    "                return x.split()\n",
    "            return []\n",
    "        corpus = df['cleaned_tokens'].apply(to_tokens).apply(lambda toks: ' '.join(toks)).tolist()\n",
    "        text_col = 'cleaned_tokens'\n",
    "    elif 'cleaned_text' in df.columns:\n",
    "        corpus = df['cleaned_text'].astype(str).tolist()\n",
    "        text_col = 'cleaned_text'\n",
    "    else:\n",
    "        # Try to infer\n",
    "        cand = None\n",
    "        for c in df.columns:\n",
    "            if df[c].dtype == object:\n",
    "                avg_len = df[c].astype(str).str.len().mean()\n",
    "                if avg_len > 10:\n",
    "                    cand = c; break\n",
    "        if cand is None:\n",
    "            raise ValueError(\"No usable text column found. Expected 'cleaned_tokens' or 'cleaned_text'.\")\n",
    "        corpus = df[cand].astype(str).tolist()\n",
    "        text_col = cand\n",
    "    return corpus, text_col\n",
    "\n",
    "corpus, text_col = build_corpus(df)\n",
    "print(\"Using text column:\", text_col)\n",
    "\n",
    "# TF-IDF\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=4000,       # you can raise this if you have a lot of data\n",
    "    ngram_range=(1, 2),      # unigrams + bigrams to catch short phrases\n",
    "    min_df=2,                # ignore ultra-rare tokens\n",
    "    token_pattern=r\"(?u)\\b\\w+\\b\"\n",
    ")\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"TF-IDF matrix shape:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3457c388",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Quick look: top TF-IDF terms globally\n",
    "global_tfidf = np.asarray(X.sum(axis=0)).ravel()\n",
    "topn = 20\n",
    "top_idx = np.argsort(global_tfidf)[-topn:][::-1]\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.barh(range(topn), global_tfidf[top_idx][::-1])\n",
    "plt.yticks(range(topn), terms[top_idx][::-1])\n",
    "plt.title(\"Top TF-IDF terms (global)\")\n",
    "plt.xlabel(\"TF-IDF sum\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c286bf",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3 — K-Means & Interpreting Results\n",
    "\n",
    "### 3A. Elbow + Helpful Metrics\n",
    "Besides inertia (elbow), we show:\n",
    "- **Silhouette** (higher is better)\n",
    "- **Calinski–Harabasz** (higher is better)\n",
    "- **Davies–Bouldin** (lower is better)\n",
    "\n",
    "This helps when the elbow looks ambiguous.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20105ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "# Reduce dimensionality for fast metrics (and later plots)\n",
    "svd_dim = min(50, max(2, X.shape[1]-1))\n",
    "svd = TruncatedSVD(n_components=svd_dim, random_state=42)\n",
    "X_svd = svd.fit_transform(X)\n",
    "\n",
    "k_values = list(range(2, 9))\n",
    "records, inertias = [], []\n",
    "\n",
    "for k in k_values:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = km.fit_predict(X)\n",
    "    inertias.append(km.inertia_)\n",
    "    try:\n",
    "        sil = silhouette_score(X_svd, labels)\n",
    "    except Exception:\n",
    "        sil = float('nan')\n",
    "    try:\n",
    "        ch = calinski_harabasz_score(X_svd, labels)\n",
    "    except Exception:\n",
    "        ch = float('nan')\n",
    "    try:\n",
    "        db = davies_bouldin_score(X_svd, labels)\n",
    "    except Exception:\n",
    "        db = float('nan')\n",
    "    records.append({\n",
    "        'k': k,\n",
    "        'inertia': km.inertia_,\n",
    "        'silhouette (SVD)': sil,\n",
    "        'calinski_harabasz (SVD)': ch,\n",
    "        'davies_bouldin (SVD)': db,\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(records)\n",
    "display(metrics_df.style.background_gradient(axis=0))\n",
    "\n",
    "# Elbow plot\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(k_values, inertias, marker='o')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Silhouette plot\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(k_values, metrics_df['silhouette (SVD)'], marker='o')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Silhouette (SVD space)')\n",
    "plt.title('Silhouette vs k')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Try automatic knee detection (optional)\n",
    "auto_k = None\n",
    "try:\n",
    "    from kneed import KneeLocator\n",
    "    kl = KneeLocator(k_values, inertias, curve='convex', direction='decreasing')\n",
    "    auto_k = kl.elbow\n",
    "    print(\"Auto-selected k (elbow):\", auto_k)\n",
    "except Exception as e:\n",
    "    print(\"KneeLocator not available/failed:\", e)\n",
    "\n",
    "# Choose k\n",
    "optimal_k = int(auto_k) if auto_k is not None else 3\n",
    "print(\"Using k =\", optimal_k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbaf542",
   "metadata": {},
   "source": [
    "\n",
    "### 3B. Run K-Means + Inspect Cluster Keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b387b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kmeans = KMeans(n_clusters=int(optimal_k), random_state=42, n_init=10)\n",
    "df['kmeans_label'] = kmeans.fit_predict(X)\n",
    "\n",
    "# Top terms per cluster (printed + bar plot)\n",
    "for i in range(int(optimal_k)):\n",
    "    center = kmeans.cluster_centers_[i]\n",
    "    top_idx = center.argsort()[-10:][::-1]\n",
    "    print(f\"Cluster {i}: \" + \", \".join([f\"{terms[j]}({center[j]:.3f})\" for j in top_idx]))\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.barh(range(10), center[top_idx][::-1])\n",
    "    plt.yticks(range(10), terms[top_idx][::-1])\n",
    "    plt.title(f\"Cluster {i}: top TF-IDF terms\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cdc3a7",
   "metadata": {},
   "source": [
    "\n",
    "### 3C. Visualize K-Means (2D)\n",
    "We use **TruncatedSVD(2)** (works well for sparse TF‑IDF) to project to 2D.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2660d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "svd2 = TruncatedSVD(n_components=2, random_state=42)\n",
    "X_2d = svd2.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "scatter = plt.scatter(X_2d[:,0], X_2d[:,1], c=df['kmeans_label'], alpha=0.6)\n",
    "plt.title('K-Means Cluster Visualization (SVD 2D)')\n",
    "plt.xlabel('Component 1'); plt.ylabel('Component 2')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show 3 representative comments closest to each cluster center (in 2D)\n",
    "import numpy as np\n",
    "examples = {}\n",
    "for i in range(int(optimal_k)):\n",
    "    idxs = np.where(df['kmeans_label'].values==i)[0]\n",
    "    if len(idxs)==0:\n",
    "        continue\n",
    "    centroid = X_2d[idxs].mean(axis=0)\n",
    "    dists = np.linalg.norm(X_2d[idxs] - centroid, axis=1)\n",
    "    top = idxs[np.argsort(dists)[:3]]\n",
    "    print(f\"\\nCluster {i} sample comments:\")\n",
    "    for t in df.iloc[top][text_col].tolist():\n",
    "        print(\"-\", shorten(str(t).replace(\"\\n\",\" \"), width=200))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb8df41",
   "metadata": {},
   "source": [
    "\n",
    "**Critical Prompt #2:**  \n",
    "State your chosen **k** and **justify it** (refer to elbow/metrics).  \n",
    "Give **one good** cluster and **one bad/confusing** cluster, with reasoning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5333714",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4 — DBSCAN (Comparative Analysis)\n",
    "\n",
    "Key ideas:\n",
    "- `eps`: neighborhood radius  \n",
    "- `min_samples`: minimum points to form a cluster (start with 5)\n",
    "\n",
    "We **scale** the sparse TF‑IDF with `StandardScaler(with_mean=False)` and try multiple `eps` values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd4f032",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Scale sparse matrix directly\n",
    "X_scaled = StandardScaler(with_mean=False).fit_transform(X)\n",
    "\n",
    "eps_candidates = [0.3, 0.5, 0.7, 0.9, 1.2]\n",
    "results = []\n",
    "labels_map = {}\n",
    "\n",
    "for eps in eps_candidates:\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=5, n_jobs=-1)\n",
    "    labels = dbscan.fit_predict(X_scaled)\n",
    "    labels_map[eps] = labels\n",
    "\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = int((labels == -1).sum())\n",
    "\n",
    "    # silhouette only if at least 2 clusters\n",
    "    try:\n",
    "        from sklearn.metrics import silhouette_score\n",
    "        sil = silhouette_score(X_svd, labels) if n_clusters > 1 else float('nan')\n",
    "    except Exception:\n",
    "        sil = float('nan')\n",
    "\n",
    "    results.append({'eps': eps, 'min_samples': 5, 'clusters': n_clusters, 'noise_points': n_noise, 'silhouette (SVD)': sil})\n",
    "\n",
    "res_df = pd.DataFrame(results)\n",
    "display(res_df.style.background_gradient(axis=0))\n",
    "\n",
    "# Choose best eps by silhouette (fallback to 0.5 if all NaN)\n",
    "if res_df['silhouette (SVD)'].notna().any():\n",
    "    best_eps = float(res_df.sort_values(['silhouette (SVD)'], ascending=False).iloc[0]['eps'])\n",
    "else:\n",
    "    best_eps = 0.5\n",
    "\n",
    "print(\"Chosen eps:\", best_eps)\n",
    "dbscan_final = DBSCAN(eps=best_eps, min_samples=5, n_jobs=-1)\n",
    "final_labels = dbscan_final.fit_predict(X_scaled)\n",
    "df['dbscan_label'] = final_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a06e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize DBSCAN on the same 2D projection (X_2d)\n",
    "plt.figure(figsize=(8,6))\n",
    "scatter = plt.scatter(X_2d[:,0], X_2d[:,1], c=df['dbscan_label'], alpha=0.6)\n",
    "plt.title(f'DBSCAN Visualization (eps={best_eps}, -1 is Noise)')\n",
    "plt.xlabel('Component 1'); plt.ylabel('Component 2')\n",
    "plt.colorbar(scatter, label='Cluster/Noise')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "def counts(d):\n",
    "    u, c = np.unique(d, return_counts=True)\n",
    "    return dict(zip(u, c))\n",
    "\n",
    "print(\"K-Means cluster counts:\", counts(df['kmeans_label'].values))\n",
    "print(\"DBSCAN cluster/noise counts:\", counts(df['dbscan_label'].values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a848b3",
   "metadata": {},
   "source": [
    "\n",
    "**Critical Prompt #3:**  \n",
    "Compare **K-Means vs DBSCAN** on your data. Which gave more **useful insight**, and why?  \n",
    "Describe a scenario where **DBSCAN** would be **better** than K-Means for comment analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3df0c03",
   "metadata": {},
   "source": [
    "\n",
    "## Step 5 — Final Reflection & Application\n",
    "\n",
    "**Critical Prompt #4:**  \n",
    "How did your dataset's characteristics (size, topic, language style) influence results?  \n",
    "If an airline or marketing company used this on customer feedback, what's the **single most important lesson** from this lab you'd tell them?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03243d77",
   "metadata": {},
   "source": [
    "\n",
    "## Export Results\n",
    "Save a CSV with the original text + labels so you can analyze clusters elsewhere.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77339112",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Export labeled data\n",
    "keep_cols = [text_col] + [c for c in ['kmeans_label','dbscan_label'] if c in df.columns]\n",
    "out_path = '/content/CSE477_lab5_clustered_comments.csv'\n",
    "df[keep_cols].to_csv(out_path, index=False)\n",
    "print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed528a4e",
   "metadata": {},
   "source": [
    "\n",
    "## (Optional) UMAP 2D (Often prettier than SVD/PCA)\n",
    "Run this if you want an alternative 2D view.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c3cff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    import umap\n",
    "    umap_2d = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1).fit_transform(X_svd)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sc = plt.scatter(umap_2d[:,0], umap_2d[:,1], c=df['kmeans_label'], alpha=0.6)\n",
    "    plt.title('UMAP 2D — colored by K-Means')\n",
    "    plt.colorbar(sc, label='Cluster')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(\"UMAP not available:\", e)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}